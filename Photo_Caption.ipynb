import warnings
warnings.filterwarnings(action='ignore')

from transformers import pipeline
from transformers.utils import logging
logging.set_verbosity(40)

from IPython.display import Image, display
import os

# Download images
!gdown 1b47sxG6Z7W8JiubMKSN38W30cbRzbTAN -O image1.jpg
!gdown 17MwA0uk4T8k9-z3fC43Qvibmxv5-EwNK -O image2.jpg
!gdown 1K-UIDdqQE62hFYe9HQWNWaHG1K30tJ7K -O image3.jpg

# File names
images = ["image1.jpg", "image2.jpg", "image3.jpg"]

# Load different captioning models
captioner1 = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning")
captioner2 = pipeline("image-to-text", model="Salesforce/blip-image-captioning-base")
captioner3 = pipeline("image-to-text", model="Salesforce/blip2-opt-2.7b")

# Function to generate captions
def generate_captions(image, captioner, model_name):
    display(Image(filename=image))
    caption = captioner(image)
    print(f"{model_name}: {caption[0]['generated_text']}")
    return caption[0]['generated_text']

# Generate captions for each image using all models
results = {}
for img in images:
    print(f"\nResults for {img}:")
    results[img] = {
        "Vit-GPT2": generate_captions(img, captioner1, "Vit-GPT2"),
        "BLIP": generate_captions(img, captioner2, "BLIP"),
        "BLIP2": generate_captions(img, captioner3, "BLIP2")
    }

# Summary of results
print("\nSummary of results:")
for img, captions in results.items():
    print(f"\n{img}:")
    for model, caption in captions.items():
        print(f"{model}: {caption}")
